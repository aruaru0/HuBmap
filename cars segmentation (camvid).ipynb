{"cells":[{"cell_type":"code","source":["# !pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n","!pip install -U segmentation-models-pytorch==0.2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Puk87wMDAKs7","executionInfo":{"status":"ok","timestamp":1658629790951,"user_tz":-540,"elapsed":9540,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"399ba161-d982-4dee-c3bc-ae21775a175d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting segmentation-models-pytorch==0.2.0\n","  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 2.6 MB/s \n","\u001b[?25hCollecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 17.7 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.13.0+cu113)\n","Collecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n","\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n","  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (1.12.0+cu113)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (1.24.3)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=7ed10067b78ba6852aa4f8dace3988d15c5652a5bdf55e1179617f1121e5dea4\n","  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=945b414e194a691c522eef36d109794f980be0894845300e0d649ecafcba7803\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n"]}]},{"cell_type":"code","source":["!pip freeze | grep segmentation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dx_qOFB9ANNd","executionInfo":{"status":"ok","timestamp":1658629792227,"user_tz":-540,"elapsed":1281,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"21e3594f-8901-47fc-80d2-b78e807c0faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["segmentation-models-pytorch==0.2.0\n"]}]},{"cell_type":"code","source":["!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wK1F2TtSAQev","executionInfo":{"status":"ok","timestamp":1658629808684,"user_tz":-540,"elapsed":16463,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"0ce58d88-bd46-4ce7-d126-55202cf1f7e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/albu/albumentations\n","  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-p18nh8__\n","  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-p18nh8__\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (1.7.3)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (3.13)\n","Collecting qudida>=0.0.4\n","  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.2.1) (4.6.0.66)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.1.1)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (1.0.2)\n","Collecting opencv-python-headless>=4.0.1\n","  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n","\u001b[K     |████████████████████████████████| 48.3 MB 10.3 MB/s \n","\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.6.3)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (3.2.2)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (7.1.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.4.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (1.3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2021.11.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.2.1) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (3.1.0)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-1.2.1-py3-none-any.whl size=116794 sha256=a8c9fb995f7771a706d1e7ccedb873e780f55f5902c87a5c6b793e0c7b7b75a3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-t1o1sn6j/wheels/63/11/1a/c77caf3ae9b9b6d57b3ee5e6a41a50f3bc12c66a70f6b90bf0\n","Successfully built albumentations\n","Installing collected packages: opencv-python-headless, qudida, albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-1.2.1 opencv-python-headless-4.6.0.66 qudida-0.0.4\n"]}]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset as BaseDataset\n","\n","import segmentation_models_pytorch as smp\n","import albumentations as albu"],"metadata":{"id":"9THOfOJdAWAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_DIR = './data/CamVid/'\n","\n","# load repo with data if it is not exists\n","if not os.path.exists(DATA_DIR):\n","    print('Loading data...')\n","    os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n","    print('Done!')\n","\n","x_train_dir = os.path.join(DATA_DIR, 'train')\n","y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n","\n","x_valid_dir = os.path.join(DATA_DIR, 'val')\n","y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n","\n","x_test_dir = os.path.join(DATA_DIR, 'test')\n","y_test_dir = os.path.join(DATA_DIR, 'testannot')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7XIEcpFAc3u","executionInfo":{"status":"ok","timestamp":1658629827874,"user_tz":-540,"elapsed":12913,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"e14b5b8e-2103-4084-aa45-4099c273ead5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Done!\n"]}]},{"cell_type":"code","source":["def get_training_augmentation():\n","    train_transform = [\n","        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n","        albu.RandomCrop(height=320, width=320, always_apply=True),\n","    ]\n","    return albu.Compose(train_transform)\n","\n","def get_validation_augmentation():\n","    \"\"\"画像のshapeが32で割り切れるようにPaddingするための関数\"\"\"\n","    test_transform = [\n","        albu.PadIfNeeded(384, 480)\n","    ]\n","    return albu.Compose(test_transform)\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose    \n","    \"\"\"\n","    \n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return albu.Compose(_transform)\n","\n","# 可視化用の関数\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()"],"metadata":{"id":"o5G_T0xxAeyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. torch.utils.data.Datasetを継承したDataset classを定義\n","class Dataset(torch.utils.data.Dataset):\n","    CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n","               'tree', 'signsymbol', 'fence', 'car', \n","               'pedestrian', 'bicyclist', 'unlabelled']\n","    \n","    def __init__(\n","            self, \n","            images_dir, # 画像のPath\n","            masks_dir, # マスク画像のPath\n","            classes=None, # 推論対象のクラス\n","            augmentation=None, # augmentation用関数\n","            preprocessing=None, # 前処理用関数\n","    ):\n","        self.ids = os.listdir(images_dir)\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n","        \n","        # クラス名の文字列('car', 'sky'など)をIDに変換\n","        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","    \n","    # 3. 学習用データ(image)と特徴(mask)を返す__getitem__メソッドを作成\n","    def __getitem__(self, i):\n","        \n","        # データの読み込み\n","        image = cv2.imread(self.images_fps[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask = cv2.imread(self.masks_fps[i], 0)\n","        \n","        # 学習対象のクラス(例えば、'car')のみを抽出\n","        masks = [(mask == v) for v in self.class_values]\n","        mask = np.stack(masks, axis=-1).astype('float')\n","        \n","        # augmentation関数の適用\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # 前処理関数の適用\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","            \n","        return image, mask\n","    # 4. データセットの長さを返す__len__を作成\n","    def __len__(self):\n","        return len(self.ids)"],"metadata":{"id":"AVevXqm4Ag4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# パラメータ\n","# ENCODER = 'se_resnext50_32x4d' # バックボーンネットワークの指定\n","ENCODER = 'resnet50' # バックボーンネットワークの指定\n","ENCODER_WEIGHTS = 'imagenet' # 使用する学習済みモデル\n","ACTIVATION = 'sigmoid' # 恒等関数や、マルチクラス用に'softmax2d'にする場合は'None'へ\n","CLASSES = ['car']\n","DEVICE = 'cuda'\n","\n","# SMPを用いて学習済みモデルを取得(アーキテクチャはFPN)\n","model = smp.FPN(\n","    encoder_name=ENCODER, \n","    encoder_weights=ENCODER_WEIGHTS, \n","    classes=len(CLASSES), \n","    activation=ACTIVATION,\n",")\n","\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["196d662a290744b2b6e9977635b1a634","b939f81b71f746e29372829ec0fc999e","b1f7db6d5a384ccc85d8039d1d04da1e","6b20a3a1cf154a9c8c034066c058cd6f","1f45eaa0b26d441784bb6fd7a0e05f33","fd634527e7664be1a344f45e828fefdd","5fd9266425414c24a80c53007720273f","4de92888817f4a01a96f5365c5706465","ad79396f0dd44a23b63107e2cfcbf5f3","ec6685cfb60648419be7030bfb50f37e","495092ebc7dd47c78de6e14766292854"]},"id":"W-2KFsbYBs-6","executionInfo":{"status":"ok","timestamp":1658629831756,"user_tz":-540,"elapsed":3884,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"b333ac21-761e-4af2-e4e9-a6ce64b39ed4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196d662a290744b2b6e9977635b1a634"}},"metadata":{}}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","# データセットのインスタンスを作成\n","train_dataset = Dataset(\n","    x_train_dir, \n","    y_train_dir, \n","    augmentation=get_training_augmentation(), \n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    classes=CLASSES,\n",")\n","\n","# データローダーの作成\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=12)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wu4b7IwnAjYI","executionInfo":{"status":"ok","timestamp":1658629831756,"user_tz":-540,"elapsed":17,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"6bc168e2-9811-42a4-fd8e-e0f3fa767609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXF5C59kAlQq","executionInfo":{"status":"ok","timestamp":1658629831756,"user_tz":-540,"elapsed":13,"user":{"displayName":"aru aru","userId":"06248374832762368521"}},"outputId":"7144af64-b56b-4583-c064-c926d3db255e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FPN(\n","  (encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (decoder): FPNDecoder(\n","    (p5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (p4): FPNBlock(\n","      (skip_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (p3): FPNBlock(\n","      (skip_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (p2): FPNBlock(\n","      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (seg_blocks): ModuleList(\n","      (0): SegmentationBlock(\n","        (block): Sequential(\n","          (0): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","          (1): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","          (2): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (1): SegmentationBlock(\n","        (block): Sequential(\n","          (0): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","          (1): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (2): SegmentationBlock(\n","        (block): Sequential(\n","          (0): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","      (3): SegmentationBlock(\n","        (block): Sequential(\n","          (0): Conv3x3GNReLU(\n","            (block): Sequential(\n","              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n","              (2): ReLU(inplace=True)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (merge): MergeBlock()\n","    (dropout): Dropout2d(p=0.2, inplace=True)\n","  )\n","  (segmentation_head): SegmentationHead(\n","    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n","    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n","    (2): Activation(\n","      (activation): Sigmoid()\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-oeG2tUgtciY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 損失関数\n","loss = smp.utils.losses.DiceLoss() \n","\n","# 評価関数\n","metrics = [\n","    smp.utils.metrics.IoU(threshold=0.5),\n","]\n","\n","# 最適化関数\n","optimizer = torch.optim.Adam([ \n","    dict(params=model.parameters(), lr=0.0001),\n","])"],"metadata":{"id":"y0UXWgHUCFdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = "],"metadata":{"id":"-maCU17l1p_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SMPに用意されているシンプルなループ関数(Train用)\n","train_epoch = smp.utils.train.TrainEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    optimizer=optimizer,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","# SMPに用意されているシンプルなループ関数(Validation用)\n","valid_epoch = smp.utils.train.ValidEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","# 学習ループの実行\n","n_epoch = 40 # エポック数\n","max_score = 0\n","for i in range(0, n_epoch):    \n","    print('\\nEpoch: {}'.format(i))\n","    train_logs = train_epoch.run(train_loader)\n","    valid_logs = valid_epoch.run(valid_loader)\n","    \n","    # 評価関数の値が更新されたらモデルを保存\n","    if max_score < valid_logs['iou_score']:\n","        max_score = valid_logs['iou_score']\n","        torch.save(model, './best_model.pth')\n","        print('Model saved!')\n","\n","    # エポック25以降は学習率(learning rate)を下げる      \n","    if i == 25:\n","        optimizer.param_groups[0]['lr'] = 1e-5\n","        print('Decrease decoder learning rate to 1e-5!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iizlJNkCNKz","outputId":"0aca5b41-8269-46e7-815f-634b6227c5cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0\n","train:   0%|          | 0/46 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["train:  83%|████████▎ | 38/46 [00:14<00:01,  7.17it/s, dice_loss - 0.3491, iou_score - 0.5107]"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"sXieTZuFCPax"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"cars segmentation (camvid).ipynb","provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"196d662a290744b2b6e9977635b1a634":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b939f81b71f746e29372829ec0fc999e","IPY_MODEL_b1f7db6d5a384ccc85d8039d1d04da1e","IPY_MODEL_6b20a3a1cf154a9c8c034066c058cd6f"],"layout":"IPY_MODEL_1f45eaa0b26d441784bb6fd7a0e05f33"}},"b939f81b71f746e29372829ec0fc999e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd634527e7664be1a344f45e828fefdd","placeholder":"​","style":"IPY_MODEL_5fd9266425414c24a80c53007720273f","value":"100%"}},"b1f7db6d5a384ccc85d8039d1d04da1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4de92888817f4a01a96f5365c5706465","max":102502400,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad79396f0dd44a23b63107e2cfcbf5f3","value":102502400}},"6b20a3a1cf154a9c8c034066c058cd6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec6685cfb60648419be7030bfb50f37e","placeholder":"​","style":"IPY_MODEL_495092ebc7dd47c78de6e14766292854","value":" 97.8M/97.8M [00:00&lt;00:00, 168MB/s]"}},"1f45eaa0b26d441784bb6fd7a0e05f33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd634527e7664be1a344f45e828fefdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fd9266425414c24a80c53007720273f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4de92888817f4a01a96f5365c5706465":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad79396f0dd44a23b63107e2cfcbf5f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec6685cfb60648419be7030bfb50f37e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"495092ebc7dd47c78de6e14766292854":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}