{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"unet-model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"10SkkgHLHQU1UG4WtvhDurXtwOEtyyZpq","authorship_tag":"ABX9TyOJ2yD8kvG8wVRt2EP1z7h3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"6VlAPCUcHLUr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## kaggle"],"metadata":{"id":"ASsJgyS1HhG0"}},{"cell_type":"code","source":["!mkdir -p ~/.kaggle\n","!cp /content/drive/MyDrive/datas/kaggle.json  ~/.kaggle/"],"metadata":{"id":"ZusRVeCNHLSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!chmod 600 /root/.kaggle/kaggle.json"],"metadata":{"id":"rSkX6Jn8He2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download -d thedevastator/hubmap-2022-256x256"],"metadata":{"id":"o2RkHCL0HLPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !mkdir hubmap\n","!unzip /content/hubmap-2022-256x256.zip -d hubmap >/dev/null"],"metadata":{"id":"ZfxiyyzRHWLz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pytorch-lightning"],"metadata":{"id":"N7VM_RDiHWJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"zTy9jckIhog_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Start code"],"metadata":{"id":"_uIQqLKMHWq8"}},{"cell_type":"code","source":["!pip install timm"],"metadata":{"id":"uB6Kgk4HquWC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lye6xTIKqfnB"},"outputs":[],"source":["from typing import Optional, List\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import ConstantLR, LinearLR\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset as BaseDataset\n","import torchvision.transforms as transforms\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","\n","from timm import create_model\n","\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","import os\n","import albumentations as albu\n","import random\n","\n","import pandas as pd\n","\n","from sklearn.model_selection import KFold"]},{"cell_type":"markdown","source":["##  load test data"],"metadata":{"id":"lLvVGPvusezx"}},{"cell_type":"code","source":["# DATA_DIR = './data/CamVid/'\n","\n","# # load repo with data if it is not exists\n","# if not os.path.exists(DATA_DIR):\n","#     print('Loading data...')\n","#     os.system('git clone https://github.com/alexgkendall/SegNet-Tutorial ./data')\n","#     print('Done!')\n","\n","# x_train_dir = os.path.join(DATA_DIR, 'train')\n","# y_train_dir = os.path.join(DATA_DIR, 'trainannot')\n","\n","# x_valid_dir = os.path.join(DATA_DIR, 'val')\n","# y_valid_dir = os.path.join(DATA_DIR, 'valannot')\n","\n","# x_test_dir = os.path.join(DATA_DIR, 'test')\n","# y_test_dir = os.path.join(DATA_DIR, 'testannot')"],"metadata":{"id":"R9xn0aSJsXqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 43\n","BATCH_SIZE = 64"],"metadata":{"id":"Oh0YDCPOLN-_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    #the following line gives ~10% speedup\n","    #but may lead to some stochasticity in the results \n","    torch.backends.cudnn.benchmark = True\n","    \n","seed_everything(SEED)"],"metadata":{"id":"fqR95HbQLMVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_DIR = './hubmap'\n","NFOLD = 5\n","\n","x_train_dir = os.path.join(DATA_DIR, 'train')\n","y_train_dir = os.path.join(DATA_DIR, 'masks')\n","y_train_dir"],"metadata":{"id":"Gjqrb5fJICDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ids = os.listdir(os.path.join(DATA_DIR, 'train'))\n","kf = KFold(n_splits=NFOLD,random_state=SEED,shuffle=True)\n","\n","df = pd.DataFrame(ids, columns=['filename'])\n","for train, test in kf.split(ids) :\n","  train_file = df.iloc[train]['filename'].to_list()\n","  test_file = df.iloc[test]['filename'].to_list()\n","  break\n"],"metadata":{"id":"a1JUdruc1wvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_file[:10]"],"metadata":{"id":"CfuZaUQa2Yi5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"8nxJQEDNsoL9"}},{"cell_type":"code","source":["WIDTH, HEIGHT = 256, 256"],"metadata":{"id":"WGu9-XADI-cB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_training_augmentation2(p=1.0):\n","    return albu.Compose([\n","        albu.HorizontalFlip(),\n","        albu.VerticalFlip(),\n","        albu.RandomRotate90(),\n","        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n","                         border_mode=cv2.BORDER_REFLECT),\n","        albu.OneOf([\n","            albu.OpticalDistortion(p=0.3),\n","            albu.GridDistortion(p=.1),\n","            albu.IAAPiecewiseAffine(p=0.3),\n","        ], p=0.3),\n","        albu.OneOf([\n","            # albu.HueSaturationValue(10,15,10),\n","            albu.CLAHE(clip_limit=2),\n","            albu.RandomBrightnessContrast(),            \n","        ], p=0.3),\n","        albu.GaussNoise(var_limit=(10.0, 50.0), p=0.3)\n","    ], p=p)\n","\n","def get_training_augmentation():\n","    train_transform = [\n","        albu.PadIfNeeded(min_height=HEIGHT, min_width=WIDTH, always_apply=True, border_mode=0),\n","        albu.RandomCrop(height=HEIGHT, width=WIDTH, always_apply=True),\n","    ]\n","    return albu.Compose(train_transform)\n","\n","\n","def get_grayaug():\n","    train_transform = [\n","        albu.ToGray(p=1.0),\n","    ]\n","    return albu.Compose(train_transform)\n","\n","# def get_validation_augmentation():\n","#     \"\"\"画像のshapeが32で割り切れるようにPaddingするための関数\"\"\"\n","#     test_transform = [\n","#         albu.PadIfNeeded(384, 480)\n","#     ]\n","#     return albu.Compose(test_transform)\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform    \n","    Args:\n","        preprocessing_fn (callbale): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose    \n","    \"\"\"\n","    \n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return albu.Compose(_transform)\n","\n","# 可視化用の関数\n","def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(16, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image)\n","    plt.show()"],"metadata":{"id":"JB4m5SOlsqLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataset"],"metadata":{"id":"Wkop_SZIstyJ"}},{"cell_type":"code","source":["mean = np.array([0.7720342, 0.74582646, 0.76392896])\n","std = np.array([0.24745085, 0.26182273, 0.25782376])\n","\n","def img2tensor(img,dtype:np.dtype=np.float32):\n","    if img.ndim==2 : img = np.expand_dims(img,2)\n","    img = np.transpose(img,(2,0,1))\n","    return torch.from_numpy(img.astype(dtype, copy=False))\n","\n","\n","# 1. torch.utils.data.Datasetを継承したDataset classを定義\n","class Dataset(torch.utils.data.Dataset):\n","    # CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n","    #            'tree', 'signsymbol', 'fence', 'car', \n","    #            'pedestrian', 'bicyclist', 'unlabelled']\n","    \n","    def __init__(\n","            self, \n","            images_dir, # 画像のPath\n","            masks_dir, # マスク画像のPath\n","            file_list, # ファイル名一覧\n","            # classes=None, # 推論対象のクラス\n","            augmentation=None, # augmentation用関数\n","            preprocessing=None, # 前処理用関数\n","    ):\n","        self.ids = file_list\n","        # self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","        # self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n","        self.images_dir = images_dir\n","        self.masks_dir = masks_dir\n","        # クラス名の文字列('car', 'sky'など)をIDに変換\n","        # self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n","        \n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","        self.to_gray = get_grayaug()\n","    \n","    # 3. 学習用データ(image)と特徴(mask)を返す__getitem__メソッドを作成\n","    def __getitem__(self, i):\n","        # データの読み込み\n","        image = cv2.imread(self.images_dir + \"/\" + self.ids[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # mask = cv2.imread(self.masks_fps[i], 0)\n","        mask = cv2.imread(self.masks_dir + \"/\" + self.ids[i], cv2.IMREAD_GRAYSCALE)\n","\n","\n","        # 学習対象のクラス(例えば、'car')のみを抽出\n","        # masks = [(mask == v) for v in self.class_values]\n","        # mask = np.stack(masks, axis=-1).astype('float')\n","        \n","        # augmentation関数の適用\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        \n","        # sample = self.to_gray(image=image, mask=mask)\n","        # image, mask = sample['image'], sample['mask']\n","\n","        # 前処理関数の適用\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","\n","        # return image, mask\n","        return img2tensor(image/255.0),img2tensor(mask)\n","        # return img2tensor((image/255.0 - mean)/std),img2tensor(mask)\n","\n","        # image = transforms.ToTensor()(image)\n","        # mask = transforms.ToTensor()(mask*255)\n","        # return image, mask\n","    # 4. データセットの長さを返す__len__を作成\n","    def __len__(self):\n","        return len(self.ids)"],"metadata":{"id":"opJRWqnAsv0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # # データセットのインスタンスを作成\n","train_dataset = Dataset(\n","    x_train_dir, \n","    y_train_dir, \n","    train_file,\n","    augmentation=get_training_augmentation(), \n",")\n","\n","print(train_dataset.__len__())\n","for i in range(10) :\n","  x,y= train_dataset[i]\n","  print(y.max())\n","  # plt.imshow(x.permute(1,2,0))\n","  break"],"metadata":{"id":"REKD_Wd6RvtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","CLASSES = ['car']\n","\n","\n","# データセットのインスタンスを作成\n","train_dataset = Dataset(\n","    x_train_dir, \n","    y_train_dir, \n","    train_file,\n","    augmentation=get_training_augmentation2(p=0.9), \n",")\n","\n","valid_dataset = Dataset(\n","    x_train_dir, \n","    y_train_dir, \n","    test_file,\n","    augmentation=None, \n",")\n","\n","# データローダーの作成\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"],"metadata":{"id":"AdjyzF8_s1lg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["row, col = 8, 8\n","plt.figure(figsize=(20,20))\n","\n","plt.subplot(col, row, 1)\n","plt.imshow(x[0])\n","plt.axis('off')\n","\n","for data, mask in train_loader:\n","  for i in range(64) :\n","    plt.subplot(col, row, i+1)\n","    x = data[i] .permute(1,2,0)\n","    plt.imshow(x)\n","    plt.axis('off')\n","  # print(data.shape)\n","  break"],"metadata":{"id":"KwoC32MVLDAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## U-net model with timm"],"metadata":{"id":"rLIllpMFqmRf"}},{"cell_type":"code","source":["\"\"\" A simple U-Net w/ timm backbone encoder\n","Based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n","Hacked together by Ross Wightman\n","\"\"\"\n","class Unet(nn.Module):\n","    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n","    Args:\n","        encoder_name: name of classification model (without last dense layers) used as feature\n","            extractor to build segmentation model.\n","        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n","        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n","        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n","            is used.\n","        num_classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n","        center: if ``True`` add ``Conv2dReLU`` block on encoder head\n","    NOTE: This is based off an old version of Unet in https://github.com/qubvel/segmentation_models.pytorch\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            backbone='resnet50',\n","            backbone_kwargs=None,\n","            backbone_indices=None,\n","            decoder_use_batchnorm=True,\n","            decoder_channels=(256, 128, 64, 32, 16),\n","            in_chans=1,\n","            num_classes=5,\n","            center=False,\n","            norm_layer=nn.BatchNorm2d,\n","    ):\n","        super().__init__()\n","        backbone_kwargs = backbone_kwargs or {}\n","        # NOTE some models need different backbone indices specified based on the alignment of features\n","        # and some models won't have a full enough range of feature strides to work properly.\n","        encoder = create_model(\n","            backbone, features_only=True, out_indices=backbone_indices, in_chans=in_chans,\n","            pretrained=True, **backbone_kwargs)\n","        encoder_channels = encoder.feature_info.channels()[::-1]\n","        self.encoder = encoder\n","\n","        if not decoder_use_batchnorm:\n","            norm_layer = None\n","        self.decoder = UnetDecoder(\n","            encoder_channels=encoder_channels,\n","            decoder_channels=decoder_channels,\n","            final_channels=num_classes,\n","            norm_layer=norm_layer,\n","            center=center,\n","        )\n","\n","    def forward(self, x: torch.Tensor):\n","        x = self.encoder(x)\n","        x.reverse()  # torchscript doesn't work with [::-1]\n","        x = self.decoder(x)\n","        return x\n","\n","\n","class Conv2dBnAct(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n","                 stride=1, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n","        self.bn = norm_layer(out_channels)\n","        self.act = act_layer(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.act(x)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, scale_factor=2.0, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n","        super().__init__()\n","        conv_args = dict(kernel_size=3, padding=1, act_layer=act_layer)\n","        self.scale_factor = scale_factor\n","        if norm_layer is None:\n","            self.conv1 = Conv2dBnAct(in_channels, out_channels, **conv_args)\n","            self.conv2 = Conv2dBnAct(out_channels, out_channels,  **conv_args)\n","        else:\n","            self.conv1 = Conv2dBnAct(in_channels, out_channels, norm_layer=norm_layer, **conv_args)\n","            self.conv2 = Conv2dBnAct(out_channels, out_channels, norm_layer=norm_layer, **conv_args)\n","\n","    def forward(self, x, skip: Optional[torch.Tensor] = None):\n","        if self.scale_factor != 1.0:\n","            x = F.interpolate(x, scale_factor=self.scale_factor, mode='nearest')\n","        if skip is not None:\n","            x = torch.cat([x, skip], dim=1)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        return x\n","\n","\n","class UnetDecoder(nn.Module):\n","\n","    def __init__(\n","            self,\n","            encoder_channels,\n","            decoder_channels=(256, 128, 64, 32, 16),\n","            final_channels=1,\n","            norm_layer=nn.BatchNorm2d,\n","            center=False,\n","    ):\n","        super().__init__()\n","\n","        if center:\n","            channels = encoder_channels[0]\n","            self.center = DecoderBlock(channels, channels, scale_factor=1.0, norm_layer=norm_layer)\n","        else:\n","            self.center = nn.Identity()\n","\n","        in_channels = [in_chs + skip_chs for in_chs, skip_chs in zip(\n","            [encoder_channels[0]] + list(decoder_channels[:-1]),\n","            list(encoder_channels[1:]) + [0])]\n","        out_channels = decoder_channels\n","\n","        self.blocks = nn.ModuleList()\n","        for in_chs, out_chs in zip(in_channels, out_channels):\n","            self.blocks.append(DecoderBlock(in_chs, out_chs, norm_layer=norm_layer))\n","        self.final_conv = nn.Conv2d(out_channels[-1], final_channels, kernel_size=(1, 1))\n","\n","        self._init_weight()\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x: List[torch.Tensor]):\n","        encoder_head = x[0]\n","        skips = x[1:]\n","        x = self.center(encoder_head)\n","        for i, b in enumerate(self.blocks):\n","            skip = skips[i] if i < len(skips) else None\n","            x = b(x, skip)\n","        x = self.final_conv(x)\n","        return x"],"metadata":{"id":"UcPBsF-eqmGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = Unet(backbone=\"dla60_res2net\", in_chans=3, num_classes=1)"],"metadata":{"id":"jOJxiMn6ql2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model.load_state_dict(torch.load('/content/drive/MyDrive/datas/unet/train_fold0.pth'))"],"metadata":{"id":"eHhBNI_hAF3B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## loss , etc"],"metadata":{"id":"FLbPsOXQtOZU"}},{"cell_type":"code","source":["def dice_loss(pred, target, smooth = 1.):\n","    pred = pred.contiguous()\n","    target = target.contiguous()\n","    intersection = (pred * target).sum(dim=2).sum(dim=2)\n","    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n","    return loss.mean()\n","\n","\n","def calc_loss(pred, target, metrics=None, bce_weight=0.5):\n","    # Dice LossとCategorical Cross Entropyを混ぜていい感じにしている\n","    bce = F.binary_cross_entropy_with_logits(pred, target)\n","    pred = torch.sigmoid(pred)\n","    dice = dice_loss(pred, target)\n","    loss = bce * bce_weight + dice * (1 - bce_weight)\n","    return loss"],"metadata":{"id":"R2nJ5962texC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  PyTroch version\n","\n","SMOOTH = 1e-6\n","\n","def dice_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n","    # You can comment out this line if you are passing tensors of equal shape\n","    # But if you are passing output from UNet or something it will most probably\n","    # be with the BATCH x 1 x H x W shape\n","    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","    labels = labels.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","\n","    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n","    union = outputs.float().sum((1,2))  +  labels.float().sum((1,2))        # Will be zzero if both are 0\n","\n","    iou = (2*intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n","    \n","    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n","    \n","    return iou.mean()\n","    # return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n","    \n","\n","def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n","    # You can comment out this line if you are passing tensors of equal shape\n","    # But if you are passing output from UNet or something it will most probably\n","    # be with the BATCH x 1 x H x W shape\n","    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","    labels = labels.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","\n","    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n","    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n","    \n","    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n","    \n","    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n","    \n","    return iou.mean()\n","    # return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n","    \n","    \n","# Numpy version\n","# Well, it's the same function, so I'm going to omit the comments\n","\n","def iou_numpy(outputs: np.array, labels: np.array):\n","    outputs = outputs.squeeze(1)\n","    \n","    intersection = (outputs & labels).sum((1, 2))\n","    union = (outputs | labels).sum((1, 2))\n","    \n","    iou = (intersection + SMOOTH) / (union + SMOOTH)\n","    \n","    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n","    \n","    return thresholded  # Or thresholded.mean()"],"metadata":{"id":"HUYQ_E1dumQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 損失関数\n","loss_fn = calc_loss\n","\n","# # 評価関数\n","# metrics = [\n","#     iou_pytorch,\n","# ]\n","\n","# # 最適化関数\n","# optimizer = torch.optim.Adam([ \n","#     dict(params=model.parameters(), lr=0.0001),\n","# ])"],"metadata":{"id":"ExgP6MOBq5_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"],"metadata":{"id":"CbJBryj31zOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LitUnet(pl.LightningModule):\n","    def __init__(self, lr=0.05):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.lr = lr\n","        # self.model = Unet(backbone=\"dla60_res2next\", in_chans=3, num_classes=1)\n","        self.model = Unet(backbone=\"dla60_res2net\", in_chans=3, num_classes=1)\n","\n","\n","    def forward(self, x):\n","        out = self.model(x)\n","        return out\n","\n","    def training_step(self, batch, batch_idx):\n","        data, label = batch\n","        pred = self.model(data)\n","        loss = loss_fn(pred, label)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        data, label = batch\n","        pred = self.model(data)\n","        loss = loss_fn(pred, label)\n","        iou = dice_pytorch(pred.cpu()>0.5, label.cpu()>0.5).detach().item()\n","\n","        self.log(f'val_loss', loss, prog_bar=True)\n","        self.log(f'val_iou', iou, prog_bar=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        data, label = batch\n","        pred = self.model(data)\n","        loss = loss_fn(pred, label)\n","        iou = dice_pytorch(pred.cpu()>0.5, label.cpu()>0.5).detach().item()\n","\n","        self.log(f'test_loss', loss, prog_bar=True)\n","        self.log(f'test_iou', iou, prog_bar=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        # optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams.lr, momentum=0.9, weight_decay=5e-4)\n","        scheduler_dict = {\n","            'scheduler': LinearLR(optimizer, start_factor=1, end_factor=0.1, total_iters=50*50), #ConstantLR(optimizer,  factor=1, total_iters=100),\n","            'interval': 'step',\n","        }\n","        return {'optimizer': optimizer, 'lr_scheduler': scheduler_dict}"],"metadata":{"id":"wnwhOMZbSMqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","checkpoint_callback = ModelCheckpoint(\n","                   dirpath=\"./\", \n","                   save_top_k=3, \n","                   monitor=\"val_iou\",\n","                   mode=\"max\",\n","               )\n","\n","# model = LitUnet(lr=0.0002)\n","\n","# trainer = pl.Trainer(\n","#     progress_bar_refresh_rate=1,\n","#     max_epochs=2,\n","#     gpus=1,\n","#     logger=pl.loggers.TensorBoardLogger('lightning_logs/', name='unet'),\n","#     callbacks=[LearningRateMonitor(logging_interval='step'),\n","#                checkpoint_callback,\n","#                ],\n","#     precision=16,\n","# )\n","\n","# checkpoint_callback = ModelCheckpoint(\n","#     save_top_k=10,\n","#     monitor=\"val_loss\",\n","#     mode=\"min\",\n","#     dirpath=\"my/path/\",\n","#     filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n","# )"],"metadata":{"id":"xL8M5PyuTBH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = LitUnet(lr=0.0002)"],"metadata":{"id":"WuMnxzsSAc8k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ids = os.listdir(os.path.join(DATA_DIR, 'train'))\n","kf = KFold(n_splits=NFOLD,random_state=SEED,shuffle=True)\n","\n","df = pd.DataFrame(ids, columns=['filename'])\n","idx = 0\n","for train, test in kf.split(ids) :\n","  train_file = df.iloc[train]['filename'].to_list()\n","  test_file = df.iloc[test]['filename'].to_list()\n","  # データセットのインスタンスを作成\n","  train_dataset = Dataset(\n","      x_train_dir, \n","      y_train_dir, \n","      train_file,\n","      augmentation=get_training_augmentation2(p=0.9), \n","  )\n","\n","  valid_dataset = Dataset(\n","      x_train_dir, \n","      y_train_dir, \n","      test_file,\n","      augmentation=None, \n","  )\n","\n","  # データローダーの作成\n","  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n","  valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n","\n","  print(\"[fold\", idx, \"]\", \"-\"*80)\n","  model = LitUnet(lr=0.0002)\n","\n","  trainer = pl.Trainer(\n","    progress_bar_refresh_rate=1,\n","    max_epochs=50,\n","    gpus=1,\n","    logger=pl.loggers.TensorBoardLogger('lightning_logs/', name='unet'),\n","    callbacks=[LearningRateMonitor(logging_interval='step'),\n","               checkpoint_callback,\n","               ],\n","    precision=16,\n","  )\n","  trainer.fit(model, \n","              train_loader,\n","              valid_loader,\n","              )\n","  model = LitUnet.load_from_checkpoint(checkpoint_path=checkpoint_callback.best_model_path)\n","  torch.save(model.model.state_dict(), \"train_fold{}.pth\".format(idx))\n","  idx += 1\n"],"metadata":{"id":"_X8wb_jTIwC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp *.pth /content/drive/MyDrive/datas/unet"],"metadata":{"id":"Jf_mg63BOcdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.model.load_state_dict(torch.load(\"train_fold0.pth\"))"],"metadata":{"id":"l3ouNbVRJ3iq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.test(\n","    model,\n","    dataloaders = valid_loader\n","    )"],"metadata":{"id":"_iGPmuJsJ3a_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qdTz9fNEJ3QE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trainer.fit(model, \n","#             train_loader,\n","#             valid_loader,\n","#             )\n","# trainer.save_checkpoint(\"unet.ckpt\")"],"metadata":{"id":"2ca3EFSGUXyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir lightning_logs/unet"],"metadata":{"id":"vyrWtJCKhM-G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from tqdm import tqdm\n","# from collections import OrderedDict\n","\n","# EPOCH = 40\n","\n","# model.to(device)\n","\n","# for i in range(EPOCH) :\n","#   model.train()\n","#   tot, cnt = 0,0\n","#   with tqdm(train_loader) as pbar:\n","#     for batch, D in enumerate(pbar):\n","#       data, label = D[0], D[1]\n","#       # print(data.shape, label.shape)\n","#       data = data.to(device)\n","#       label = label.to(device)\n","#       pred = model(data)\n","#       loss = loss_fn(pred, label)\n","#       optimizer.zero_grad()\n","#       loss.backward()\n","#       optimizer.step()\n","#       loss  = loss.detach().item()\n","#       tot += loss\n","#       cnt += 1\n","#       pbar.set_postfix(OrderedDict(loss=loss))\n","#     print(\"\")\n","#     print(\"[EPOCH {}] train loss={}\".format(i+1, tot/cnt))\n","\n","#   model.eval()\n","#   tot, cnt = 0,0\n","#   iou = 0.0\n","#   with tqdm(valid_loader) as pbar:\n","#     for batch, D in enumerate(pbar):\n","#       data, label = D[0], D[1]\n","#       # print(data.shape, label.shape)\n","#       data = data.to(device)\n","#       label = label.to(device)\n","#       pred = model(data)\n","#       loss = loss_fn(pred, label)\n","#       loss = loss.detach().item()\n","#       iou += iou_pytorch(pred.cpu()>0.5, label.cpu()>0.5).detach().item()\n","#       # print(iou)\n","#       tot += loss\n","#       cnt += 1\n","#       pbar.set_postfix(OrderedDict(loss=loss))\n","#     print(\"\")\n","#     print(\"[EPOCH {}] valid loss={} iou = {}\".format(i+1, tot/cnt, iou/cnt))\n","#     print(\"-\"*80)\n","#     print(\"\")\n","    "],"metadata":{"id":"tz8VkZ3Uq_dK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# checkpoint_callback.best_model_path\n","model = LitUnet.load_from_checkpoint(checkpoint_path=checkpoint_callback.best_model_path)"],"metadata":{"id":"w4yaOAmBrBBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.test(\n","    model,\n","    dataloaders = valid_loader\n","    )"],"metadata":{"id":"nPMxyEC-trhW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num = 102\n","x, y = valid_dataset[num]\n","x.shape\n","\n","row, col = 3,1\n","plt.figure(figsize=(20,20))\n","\n","plt.subplot(col, row, 1)\n","plt.imshow(x[0])\n","plt.axis('off')\n","\n","plt.subplot(col, row, 2)\n","plt.imshow(y[0])\n","plt.axis('off')\n","\n","y.max()"],"metadata":{"id":"eM1whUl1Ilfy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.unsqueeze(0).shape"],"metadata":{"id":"78hjNhDtxo9J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)\n","model.eval()\n","pred = model.forward(x.unsqueeze(0).to(device)).cpu()"],"metadata":{"id":"TzjYWr0l5fHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["row, col = 3,1\n","plt.figure(figsize=(20,20))\n","\n","plt.subplot(col, row, 1)\n","plt.imshow(x[0])\n","plt.axis('off')\n","\n","plt.subplot(col, row, 2)\n","plt.imshow(y[0])\n","plt.axis('off')\n","\n","plt.subplot(col, row, 3)\n","plt.imshow(pred[0][0].detach().numpy() > 0.5)\n","plt.axis('off')"],"metadata":{"id":"FUaYktSv8wqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check iou function \n","SMOOTH = 1e-6\n","def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n","    # You can comment out this line if you are passing tensors of equal shape\n","    # But if you are passing output from UNet or something it will most probably\n","    # be with the BATCH x 1 x H x W shape\n","    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n","    \n","    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n","    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n","    \n","\n","    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n","\n","    print(union, intersection, iou)\n","    \n","    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n","    \n","    return thresholded.mean(), iou.mean()  # Or thresholded.mean() if you are interested in average across the batch\n","\n","iou_pytorch(pred>0.5, y>0.5), dice_pytorch(pred>0.5, y>0.5)\n","\n"],"metadata":{"id":"fFcmzsXHxgkw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CPpVDhSnUesu"},"execution_count":null,"outputs":[]}]}