{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aruaru0/HuBmap/blob/main/SUB5_b5_train_segformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SegFormer Train"
      ],
      "metadata": {
        "id": "coXgfA_1lnzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VlAPCUcHLUr",
        "outputId": "48c2fbe6-a379-4731-ee19-002917be4b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug 25 10:39:51 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade"
      ],
      "metadata": {
        "id": "ctMm_fWUqIeu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "us-2NPTRqNud",
        "outputId": "95c4684e-1dd9-462c-aa51-b4596cf1b972"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASsJgyS1HhG0"
      },
      "source": [
        "## kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZusRVeCNHLSH"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/datas/kaggle.json  ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rSkX6Jn8He2w"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htK5UBd9TIF-",
        "outputId": "41a0f2c8-da3c-4537-9fd2-714e9d671871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hubmap-organ-segmentation.zip to /content\n",
            "100% 5.76G/5.78G [00:34<00:00, 193MB/s]\n",
            "100% 5.78G/5.78G [00:34<00:00, 181MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c hubmap-organ-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfxiyyzRHWLz"
      },
      "outputs": [],
      "source": [
        "# !mkdir hubmap\n",
        "!unzip /content/hubmap-organ-segmentation.zip -d hubmap >/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7VM_RDiHWJL"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTy9jckIhog_"
      },
      "outputs": [],
      "source": [
        "!pip install staintools\n",
        "!pip install spams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZOvcTGK5kUtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MgkzDIGekdaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sw0mbHf3Ox9"
      },
      "outputs": [],
      "source": [
        "# from google.colab import output\n",
        "# output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uIQqLKMHWq8"
      },
      "source": [
        "## Start code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"HuBMAP-SegFormer\")"
      ],
      "metadata": {
        "id": "b9B2r8jqkcPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB6Kgk4HquWC"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "NBTHId1nX9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "xl_L_D8-VGAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lye6xTIKqfnB"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ConstantLR, LinearLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# from timm import create_model\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import albumentations as albu\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRMdvYprU-Ms"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/code/pestipeti/decoding-rle-masks/notebook\n",
        "def mask2rle(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels= img.T.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "\n",
        "def rle2mask(mask_rle, shape=(3000,3000)):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (width,height) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLvVGPvusezx"
      },
      "source": [
        "##  load test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh0YDCPOLN-_"
      },
      "outputs": [],
      "source": [
        "SEED = 43\n",
        "BATCH_SIZE = 4\n",
        "Gradient_Accumulation_Step = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqR95HbQLMVQ"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    #the following line gives ~10% speedup\n",
        "    #but may lead to some stochasticity in the results \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8GuBH4qUqhq"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = './hubmap'\n",
        "NFOLD = 5\n",
        "FOLD = 1\n",
        "\n",
        "WIDTH, HEIGHT = 512,512\n",
        "\n",
        "model_path = '/content/drive/MyDrive/datas/HuBMAP/model-b5-fold{}.pth'.format(FOLD)\n",
        "MODEL_NAME = \"nvidia/mit-b5\"\n",
        "# MODEL_NAME = \"nvidia/segformer-b3-finetuned-ade-512-512\"\n",
        "\n",
        "wandb.SEED = SEED\n",
        "wandb.config.NFOLD = NFOLD\n",
        "wandb.config.FOLD = FOLD\n",
        "wandb.config.WIDTH = WIDTH\n",
        "wandb.config.HEIGHT= HEIGHT\n",
        "wandb.config.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "#\n",
        "# nvidia/segformer-b0-finetuned-ade-512-512\n",
        "# nvidia/segformer-b1-finetuned-ade-512-512\n",
        "# nvidia/segformer-b2-finetuned-ade-512-512\n",
        "# nvidia/segformer-b3-finetuned-ade-512-512\n",
        "# nvidia/segformer-b4-finetuned-ade-512-512\n",
        "# nvidia/segformer-b5-finetuned-ade-640-640\n",
        "# https://huggingface.co/models?other=segformer&sort=downloads&search=nvidia%2Fsegformer+finetuned\n",
        "#\n",
        "df = pd.read_csv(DATA_DIR + '/train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH7WmOpQVJd2"
      },
      "outputs": [],
      "source": [
        "# df = df[df['organ']=='lung'].reset_index()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OmRF7THcs4p"
      },
      "outputs": [],
      "source": [
        "# class2idx = dict([(name,i+1) for i, name in enumerate(df.organ.unique())])\n",
        "# class2idx['none'] = 0\n",
        "# idx2class = dict([(class2idx[name], name) for name in class2idx])\n",
        "\n",
        "class2idx = {'prostate': 1,\n",
        "  'spleen': 2,\n",
        "  'lung': 3,\n",
        "  'kidney': 4,\n",
        "  'largeintestine': 5,\n",
        "  'none': 0}\n",
        "idx2class = {1: 'prostate',\n",
        "  2: 'spleen',\n",
        "  3: 'lung',\n",
        "  4: 'kidney',\n",
        "  5: 'largeintestine',\n",
        "  0: 'none'}\n",
        "\n",
        "class2idx, idx2class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWhLJ56dU53f"
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "height, width, rle = df['img_height'][idx], df['img_width'][idx], df['rle'][idx]\n",
        "mask = rle2mask(rle, shape=(height, width))\n",
        "plt.imshow(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gjqrb5fJICDo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "df['fold'] = 0\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "for i, index in enumerate(skf.split(df.id, df.organ)):\n",
        "  train_index, test_index = index\n",
        "  # print(\"train_index:\", train_index, \"test_index:\", test_index)\n",
        "  # df['fold'][test_index] = i\n",
        "  df.loc[test_index, 'fold'] = i\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nxJQEDNsoL9"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGu9-XADI-cB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### augumentation"
      ],
      "metadata": {
        "id": "CoLzpfoUCpoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import staintools"
      ],
      "metadata": {
        "id": "GBnLIMcKgwCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stain(albu.ImageOnlyTransform):\n",
        "    def __init__(\n",
        "        self,\n",
        "        method='vahadane', sigma1=0.2, sigma2=0.2,\n",
        "        always_apply=False,\n",
        "        p=0.5,\n",
        "    ):\n",
        "        super(Stain, self).__init__(always_apply, p)\n",
        "        self.method = method\n",
        "        self.sigma1 = sigma1\n",
        "        self.sigma2 = sigma2\n",
        "        self.augmentor = staintools.StainAugmentor(method=method, sigma1=sigma1, sigma2=sigma2)\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        self.augmentor.fit(img)\n",
        "        return self.augmentor.pop().astype(int)\n",
        "\n",
        "    def get_params(self):\n",
        "        return {\n",
        "        }\n",
        "\n",
        "    def get_transform_init_args_names(self):\n",
        "        return (\"brightness_limit\", \"contrast_limit\", \"brightness_by_max\")"
      ],
      "metadata": {
        "id": "mV4llHQOgeAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- set0\n",
        "def get_training_augmentation_set0(p=0.5):\n",
        "    return albu.Compose([\n",
        "        albu.Sequential([\n",
        "          albu.RandomScale(scale_limit=(-0.8, 0.0), interpolation=cv2.INTER_LINEAR, p=1),\n",
        "          albu.Resize(HEIGHT, WIDTH, interpolation=cv2.INTER_LINEAR, p=1)\n",
        "        ], p=p),\n",
        "        albu.HorizontalFlip(p=p),\n",
        "        albu.VerticalFlip(p=p),\n",
        "        albu.RandomRotate90(p=p),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "                         border_mode=cv2.BORDER_REFLECT),\n",
        "        # albu.OneOf([\n",
        "        #     albu.OpticalDistortion(p=1),\n",
        "        #     albu.GridDistortion(p=1),\n",
        "        #     albu.IAAPiecewiseAffine(p=1),\n",
        "        # ], p=p),\n",
        "        albu.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=p),\n",
        "        albu.OneOf([\n",
        "            albu.CLAHE(clip_limit=2, p = 1),\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=1),\n",
        "            albu.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
        "        ], p=p),\n",
        "        albu.GaussNoise(var_limit=(10.0, 50.0), p=p),\n",
        "    ], p=p)\n",
        "# ---- set1\n",
        "def get_training_augmentation_set1(p=0.5):\n",
        "    return albu.Compose([\n",
        "        albu.Sequential([\n",
        "          albu.RandomScale(scale_limit=(-0.8, 0.0), interpolation=cv2.INTER_LINEAR, p=1),\n",
        "          albu.Resize(HEIGHT, WIDTH, interpolation=cv2.INTER_LINEAR, p=1)\n",
        "        ], p=p),\n",
        "        albu.HorizontalFlip(p=p),\n",
        "        albu.VerticalFlip(p=p),\n",
        "        albu.RandomRotate90(p=p),\n",
        "        # albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "        #                  border_mode=cv2.BORDER_REFLECT),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "                         border_mode=cv2.BORDER_REFLECT),\n",
        "        # albu.OneOf([\n",
        "        #     albu.OpticalDistortion(p=1),\n",
        "        #     albu.GridDistortion(p=1),\n",
        "        #     albu.IAAPiecewiseAffine(p=1),\n",
        "        # ], p=p),\n",
        "        albu.ChannelShuffle(p=p),\n",
        "        albu.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=p),\n",
        "        albu.OneOf([\n",
        "            albu.CLAHE(clip_limit=2, p = 1),\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=1),\n",
        "            albu.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
        "        ], p=p),\n",
        "        albu.GaussNoise(var_limit=(10.0, 50.0), p=p),\n",
        "    ], p=p)\n"
      ],
      "metadata": {
        "id": "aC8psbDCkrO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_augmentation3(p=0.5):\n",
        "    return albu.Compose([\n",
        "        albu.Sequential([\n",
        "          albu.RandomScale(scale_limit=(-0.8, 0.0), interpolation=cv2.INTER_LINEAR, p=1),\n",
        "          albu.Resize(HEIGHT, WIDTH, interpolation=cv2.INTER_LINEAR, p=1)\n",
        "        ], p=p),\n",
        "        albu.HorizontalFlip(p=p),\n",
        "        albu.VerticalFlip(p=p),\n",
        "        albu.RandomRotate90(p=p),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "                         border_mode=cv2.BORDER_REFLECT),\n",
        "        # albu.OneOf([\n",
        "        #     albu.OpticalDistortion(p=1),\n",
        "        #     albu.GridDistortion(p=1),\n",
        "        #     albu.IAAPiecewiseAffine(p=1),\n",
        "        # ], p=p),\n",
        "        albu.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=p),\n",
        "        albu.OneOf([\n",
        "            # albu.CLAHE(clip_limit=2, p = 1),\n",
        "            albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=1),\n",
        "            albu.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
        "        ], p=p),\n",
        "        albu.GaussNoise(var_limit=(10.0, 50.0), p=p),\n",
        "        # albu.CoarseDropout (max_holes=50, max_height=16, max_width=16, p=p)\n",
        "    ], p=p)\n",
        "\n",
        "# def get_training_augmentation3(p=0.5):\n",
        "#     return albu.Compose([\n",
        "#         albu.Sequential([\n",
        "#           albu.RandomScale(scale_limit=(-0.8, 0.0), interpolation=cv2.INTER_LINEAR, p=1),\n",
        "#           albu.Resize(HEIGHT, WIDTH, interpolation=cv2.INTER_LINEAR, p=1)\n",
        "#         ], p=p),\n",
        "#         albu.HorizontalFlip(p=p),\n",
        "#         albu.VerticalFlip(p=p),\n",
        "#         albu.RandomRotate90(p=p),\n",
        "#         albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "#                          border_mode=cv2.BORDER_REFLECT),\n",
        "#         # albu.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=15, p=p, \n",
        "#         #                  border_mode=cv2.BORDER_REFLECT),\n",
        "#         # albu.OneOf([\n",
        "#         #     albu.OpticalDistortion(p=1),\n",
        "#         #     albu.GridDistortion(p=1),\n",
        "#         #     albu.IAAPiecewiseAffine(p=1),\n",
        "#         # ], p=p),\n",
        "#         albu.OneOf([\n",
        "#             albu.ChannelShuffle(p=1),\n",
        "#             albu.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=1),\n",
        "#             albu.CLAHE(clip_limit=2, p = 1),\n",
        "#             albu.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=1),\n",
        "#             albu.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1),\n",
        "#         ], p=p),\n",
        "#         albu.GaussNoise(var_limit=(10.0, 50.0), p=p),\n",
        "#         albu.CoarseDropout (max_holes=50, max_height=16, max_width=16, p=p)\n",
        "#     ], p=p)\n"
      ],
      "metadata": {
        "id": "caixnsUpDOD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB4m5SOlsqLh"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation2(p=1.0):\n",
        "    return albu.Compose([\n",
        "        albu.HorizontalFlip(),\n",
        "        albu.VerticalFlip(),\n",
        "        albu.RandomRotate90(),\n",
        "        albu.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, \n",
        "                         border_mode=cv2.BORDER_REFLECT),\n",
        "        albu.OneOf([\n",
        "            albu.OpticalDistortion(p=0.3),\n",
        "            albu.GridDistortion(p=.1),\n",
        "            albu.IAAPiecewiseAffine(p=0.3),\n",
        "        ], p=0.3),\n",
        "        albu.OneOf([\n",
        "            # albu.HueSaturationValue(10,15,10),\n",
        "            albu.CLAHE(clip_limit=2),\n",
        "            albu.RandomBrightnessContrast(),            \n",
        "        ], p=0.3),\n",
        "        albu.GaussNoise(var_limit=(10.0, 50.0), p=0.3)\n",
        "    ], p=p)\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        albu.PadIfNeeded(min_height=HEIGHT, min_width=WIDTH, always_apply=True, border_mode=0),\n",
        "        albu.RandomCrop(height=HEIGHT, width=WIDTH, always_apply=True),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_grayaug():\n",
        "    train_transform = [\n",
        "        albu.ToGray(p=1.0),\n",
        "    ]\n",
        "    return albu.Compose(train_transform)\n",
        "\n",
        "# def get_validation_augmentation():\n",
        "#     \"\"\"画像のshapeが32で割り切れるようにPaddingするための関数\"\"\"\n",
        "#     test_transform = [\n",
        "#         albu.PadIfNeeded(384, 480)\n",
        "#     ]\n",
        "#     return albu.Compose(test_transform)\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn):\n",
        "    \"\"\"Construct preprocessing transform    \n",
        "    Args:\n",
        "        preprocessing_fn (callbale): data normalization function \n",
        "            (can be specific for each pretrained neural network)\n",
        "    Return:\n",
        "        transform: albumentations.Compose    \n",
        "    \"\"\"\n",
        "    \n",
        "    _transform = [\n",
        "        albu.Lambda(image=preprocessing_fn),\n",
        "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return albu.Compose(_transform)\n",
        "\n",
        "# 可視化用の関数\n",
        "def visualize(**images):\n",
        "    \"\"\"PLot images in one row.\"\"\"\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wkop_SZIstyJ"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9BEY5tVYt28"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "mean = np.array([0.7720342, 0.74582646, 0.76392896])\n",
        "std = np.array([0.24745085, 0.26182273, 0.25782376])\n",
        "\n",
        "def img2tensor(img,dtype:np.dtype=np.float32):\n",
        "    if img.ndim==2 : img = np.expand_dims(img,2)\n",
        "    img = np.transpose(img,(2,0,1))\n",
        "    return torch.from_numpy(img.astype(dtype, copy=False))\n",
        "\n",
        "\n",
        "# 1. torch.utils.data.Datasetを継承したDataset classを定義\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    # CLASSES = ['sky', 'building', 'pole', 'road', 'pavement', \n",
        "    #            'tree', 'signsymbol', 'fence', 'car', \n",
        "    #            'pedestrian', 'bicyclist', 'unlabelled']\n",
        "    \n",
        "    def __init__(\n",
        "            self,\n",
        "            image_dir, \n",
        "            df, # ファイル名一覧\n",
        "            # classes=None, # 推論対象のクラス\n",
        "            augmentation=None, # augmentation用関数\n",
        "            preprocessing=None, # 前処理用関数\n",
        "            width = 768,\n",
        "            height = 768,\n",
        "    ):\n",
        "        self.image_dir = image_dir\n",
        "        self.df = df\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.feature_extractor = SegformerFeatureExtractor(MODEL_NAME , reduce_labels=False, size=(width, height))\n",
        "    \n",
        "    # 3. 学習用データ(image)と特徴(mask)を返す__getitem__メソッドを作成\n",
        "    def __getitem__(self, i):\n",
        "        # データの読み込み\n",
        "        fname = \"{}/{}.tiff\".format(self.image_dir,self.df['id'][i])\n",
        "        # print(fname)\n",
        "        image = cv2.imread(fname)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        rle, w, h = self.df['rle'][i], self.df['img_width'][i], self.df['img_height'][i]\n",
        "        idx = class2idx[self.df['organ'][i]]\n",
        "        mask = rle2mask(rle, shape = (w, h))\n",
        "        # print(self.df['organ'][i], idx)\n",
        "\n",
        "        image = cv2.resize(image, (self.width, self.height))\n",
        "        mask = cv2.resize(mask, (self.width, self.height)) * idx\n",
        "\n",
        "        # augmentation関数の適用\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "        # print(\"image\", image.shape)\n",
        "        # # return image, mask\n",
        "        # image = (image/255 - mean)/std\n",
        "        \n",
        "        # return img2tensor(image),torch.tensor(mask), idx\n",
        "        # return image, mask\n",
        "        encoded_inputs = self.feature_extractor(image, mask, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in encoded_inputs.items():\n",
        "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
        "\n",
        "        return encoded_inputs\n",
        "\n",
        "    # 4. データセットの長さを返す__len__を作成\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qRySkP-ZPCd"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIR = './hubmap/train_images'\n",
        "train_dataset = Dataset(IMAGE_DIR, df[df['fold'] != FOLD].reset_index(), width=WIDTH, height=HEIGHT)\n",
        "input = train_dataset[0]\n",
        "print(input['pixel_values'].shape)\n",
        "plt.imshow(input['pixel_values'].permute(1,2,0).numpy())\n",
        "plt.imshow(input['labels'], alpha=0.5)\n",
        "input['labels'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVe_IqQr5te5"
      },
      "outputs": [],
      "source": [
        "input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REKD_Wd6RvtH"
      },
      "outputs": [],
      "source": [
        "# # # データセットのインスタンスを作成\n",
        "train_dataset = Dataset(IMAGE_DIR, df[df['fold'] != 0].reset_index(), \n",
        "                        width=WIDTH, height=HEIGHT,\n",
        "                        augmentation=get_training_augmentation3(), \n",
        "                        )\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "print(train_dataset.__len__())\n",
        "for i in range(10) :\n",
        "  input = train_dataset[i]\n",
        "  print(idx, input.pixel_values.shape)\n",
        "  plt.subplot(2, 5, i+1);\n",
        "  plt.imshow(input['pixel_values'].permute(1,2,0).numpy())\n",
        "  # plt.imshow(input['labels'], alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdjyzF8_s1lg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# CLASSES = ['car']\n",
        "\n",
        "\n",
        "# データセットのインスタンスを作成\n",
        "train_dataset = Dataset(IMAGE_DIR, df[df['fold'] != FOLD].reset_index(), \n",
        "                        width=WIDTH, height=HEIGHT,\n",
        "                        augmentation=get_training_augmentation3(p=0.9), \n",
        "                        )\n",
        "\n",
        "valid_dataset = Dataset(\n",
        "    IMAGE_DIR, df[df['fold'] == FOLD].reset_index(), \n",
        "    width=WIDTH, height=HEIGHT,\n",
        "    augmentation=None, \n",
        ")\n",
        "\n",
        "# データローダーの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwoC32MVLDAI"
      },
      "outputs": [],
      "source": [
        "row, col = 8, 8\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "for inputs in train_loader:\n",
        "  print(idx)\n",
        "  data = inputs['pixel_values']\n",
        "  mask = inputs['labels']\n",
        "  for i in range(BATCH_SIZE) :\n",
        "    plt.subplot(col, row, i+1)\n",
        "    x = data[i].permute(1,2,0)\n",
        "    plt.imshow((x-x.min())/(x.max()-x.min()))\n",
        "    plt.imshow(mask[i], alpha=0.3)\n",
        "    plt.axis('off')\n",
        "  # print(data.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u-yxcByislv"
      },
      "source": [
        "## ここまで前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDqYq50YiraE"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "# define model\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME,\n",
        "                                                         num_labels=len(class2idx), \n",
        "                                                         id2label=idx2class, \n",
        "                                                         label2id=class2idx,\n",
        "                                                         ignore_mismatched_sizes=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODVV0E66MnsN"
      },
      "outputs": [],
      "source": [
        "# # model_path = '/content/drive/MyDrive/datas/HuBMAP/model.pth'\n",
        "# model_path_last = '/content/drive/MyDrive/datas/HuBMAP/last-fold{}.pth'.format(FOLD)\n",
        "# model.load_state_dict(torch.load(model_path_last))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C__szoMM-t9B"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQZmCYmBotx3"
      },
      "outputs": [],
      "source": [
        "test_df = df[df['fold'] == FOLD].reset_index()\n",
        "\n",
        "def dice_coef(mask1, mask2):\n",
        "    intersect = np.sum(mask1*mask2)\n",
        "    sum1 = np.sum(mask1)\n",
        "    sum2 = np.sum(mask2)\n",
        "    dice = 2*intersect/(sum1+sum2)\n",
        "    dice = np.mean(dice)\n",
        "    return dice\n",
        "\n",
        "def valid(model2) :\n",
        "  model2.eval()\n",
        "  dice = 0.0\n",
        "  loss = 0.0\n",
        "  for i in tqdm(range(len(test_df))) :\n",
        "    id = test_df.id[i]\n",
        "    fname = \"/content/hubmap/train_images/{}.tiff\".format(id)\n",
        "    image = Image.open(fname)\n",
        "    encoding = feature_extractor(image, return_tensors=\"pt\")\n",
        "    pixel_values = encoding.pixel_values.cuda()\n",
        "    organ = test_df.organ[i]\n",
        "\n",
        "    rle = test_df.rle[i]\n",
        "    height, width = test_df.img_height[i], test_df.img_width[i]\n",
        "    gt = rle2mask(rle, shape = (height, width))\n",
        "\n",
        "    label = torch.tensor(gt).unsqueeze(0).long().cuda()\n",
        "    # print(label.shape)\n",
        "\n",
        "    idx = class2idx[organ]\n",
        "    with torch.no_grad():\n",
        "      outputs = model2(pixel_values=pixel_values, labels=label)\n",
        "      loss += outputs.loss.item()\n",
        "      # print(outputs.loss, loss)\n",
        "      upsampled_logits = nn.functional.interpolate(outputs['logits'],\n",
        "                  # size=image.size[::-1], # (height, width)\n",
        "                  (height, width),\n",
        "                  mode='bilinear',\n",
        "                  align_corners=False)\n",
        "      mask = upsampled_logits.argmax(dim=1)[0]\n",
        "      mask[mask != idx] = 0\n",
        "      mask[mask == idx] = 1\n",
        "\n",
        "\n",
        "    dice += dice_coef(mask.cpu().numpy(), gt)\n",
        "  return dice/len(test_df), loss/len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YItZ9kku5bSe"
      },
      "outputs": [],
      "source": [
        "# valid(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdRkTHuz-K8q"
      },
      "source": [
        "## TRAIN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frCsGhUV-vm_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from sklearn.metrics import accuracy_score\n",
        "# from tqdm.notebook import tqdm\n",
        "# from tqdm import tqdm\n",
        "from fastprogress.fastprogress import  progress_bar as tqdm\n",
        "\n",
        "from PIL import Image\n",
        "from transformers import SegformerFeatureExtractor\n",
        "\n",
        "writer = SummaryWriter(log_dir=\"./logs\")\n",
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=False, size=(WIDTH, HEIGHT))\n",
        "use_amp = True # ampをオンオフ\n",
        "\n",
        "\n",
        "# define optimizer\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4,  weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma=0.75)\n",
        "\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n",
        "#                                               base_lr=1e-5,\n",
        "#                                               max_lr=1e-3,\n",
        "#                                               step_size_up=7,\n",
        "#                                               step_size_down=13,\n",
        "#                                               mode=\"triangular\",\n",
        "#                                               cycle_momentum=False,\n",
        "#                                               gamma=0.95)\n",
        "\n",
        "\n",
        "model.train()\n",
        "cur_loss = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(300):  # loop over the dataset multiple times\n",
        "   print(\"Epoch:\", epoch)\n",
        "   \n",
        "   model.train()\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   tot_loss = 0.0\n",
        "   for idx, batch in enumerate(tqdm(train_loader)):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # print(pixel_values.shape, labels.shape)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            loss, logits = outputs.loss, outputs.logits\n",
        "\n",
        "        tot_loss += loss.detach().cpu().numpy()\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (idx + 1) % Gradient_Accumulation_Step == 0 or idx == len(train_loader) :\n",
        "            # optimizer.step()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "   scheduler.step()\n",
        "\n",
        "   print(\"Loss:\", tot_loss/len(train_loader))\n",
        "\n",
        "   print(\"valid...\")\n",
        "   dice,valid_loss = valid(model)\n",
        "\n",
        "   writer.add_scalars(\"loss\", {\n",
        "       \"train\": tot_loss/len(train_loader),\n",
        "       \"valid\": valid_loss,\n",
        "       }, epoch+1)  \n",
        "   writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], epoch+1)\n",
        "   \n",
        "   wandb.log({'epoch': epoch, 'train_loss': tot_loss/len(train_loader), \"valid_loss\": valid_loss, \n",
        "              \"dice\": dice, \"lr\":scheduler.get_last_lr()[0], \"best dice\": cur_loss, \"best epoch\": best_epoch})\n",
        "   if dice > cur_loss :\n",
        "     cur_loss = dice\n",
        "     best_epoch = epoch\n",
        "     print(\"update best dice = \", dice, \"loss = \", valid_loss)\n",
        "     torch.save(model.state_dict(), model_path)\n",
        "   else :\n",
        "     print(\"epoch dice = \", dice, \"loss = \", valid_loss)\n",
        "\n",
        "\n",
        "writer.close()\n",
        "wandb.save(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_last = '/content/drive/MyDrive/datas/HuBMAP/last-fold{}.pth'.format(FOLD)\n",
        "torch.save(model.state_dict(), model_path_last)"
      ],
      "metadata": {
        "id": "fInZHXq9akiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "u9t9l7FfaJXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeH7SAbTALia"
      },
      "outputs": [],
      "source": [
        "# from transformers import SegformerForSemanticSegmentation\n",
        "# # define model\n",
        "# model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b3\",\n",
        "#                                                          num_labels=len(class2idx), \n",
        "#                                                          id2label=idx2class, \n",
        "#                                                          label2id=class2idx,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bdwT4yUWKEz"
      },
      "outputs": [],
      "source": [
        "# model2 = SegformerForSemanticSegmentation(model.config)\n",
        "# model_path = '/content/drive/MyDrive/datas/HuBMAP/model.pth'\n",
        "# model2.load_state_dict(torch.load(model_path))\n",
        "# model2 = model2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpJVVIXsWha0"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# image = Image.open('/content/hubmap/train_images/10044.tiff')\n",
        "# plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YkkmxL7W8xx"
      },
      "outputs": [],
      "source": [
        "# from transformers import SegformerFeatureExtractor\n",
        "# feature_extractor = SegformerFeatureExtractor(reduce_labels=False)\n",
        "\n",
        "# encoding = feature_extractor(image, return_tensors=\"pt\")\n",
        "# pixel_values = encoding.pixel_values.cuda()\n",
        "# print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFi-EAHfXZb5"
      },
      "outputs": [],
      "source": [
        "# # forward pass\n",
        "# outputs = model2(pixel_values=pixel_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-i-jagnXxi6"
      },
      "outputs": [],
      "source": [
        "# logits = outputs.logits.cpu()\n",
        "# print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eThG8WspYIL-"
      },
      "outputs": [],
      "source": [
        "# def ade_palette():\n",
        "#     \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
        "#     return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
        "#             [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O09TFcX2jN"
      },
      "outputs": [],
      "source": [
        "# from torch import nn\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # First, rescale logits to original image size\n",
        "# upsampled_logits = nn.functional.interpolate(logits,\n",
        "#                 size=image.size[::-1], # (height, width)\n",
        "#                 mode='bilinear',\n",
        "#                 align_corners=False)\n",
        "\n",
        "# # Second, apply argmax on the class dimension\n",
        "# seg = upsampled_logits.argmax(dim=1)[0]\n",
        "# color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "# palette = np.array(ade_palette())\n",
        "# for label, color in enumerate(palette):\n",
        "#     color_seg[seg == label, :] = color\n",
        "# # Convert to BGR\n",
        "# color_seg = color_seg[..., ::-1]\n",
        "\n",
        "# # Show image + mask\n",
        "# img = np.array(image) * 0.5 + color_seg * 0.5\n",
        "# img = img.astype(np.uint8)\n",
        "\n",
        "# plt.figure(figsize=(15, 10))\n",
        "# plt.imshow(img)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7KXGkPRsMF_"
      },
      "source": [
        "# inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIVlLcRYu97C"
      },
      "outputs": [],
      "source": [
        "def dice_coef(mask1, mask2):\n",
        "    intersect = np.sum(mask1*mask2)\n",
        "    sum1 = np.sum(mask1)\n",
        "    sum2 = np.sum(mask2)\n",
        "    dice = 2*intersect/(sum1+sum2)\n",
        "    dice = np.mean(dice)\n",
        "    return dice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5G_Uy7pMZd7"
      },
      "outputs": [],
      "source": [
        "from transformers import SegformerForSemanticSegmentation\n",
        "from transformers import SegformerModel, SegformerConfig\n",
        "\n",
        "# define model\n",
        "# model = SegformerForSemanticSegmentation.from_pretrained(MODEL_NAME,\n",
        "#                                                          num_labels=len(class2idx), \n",
        "#                                                          id2label=idx2class, \n",
        "#                                                          label2id=class2idx,\n",
        "# )\n",
        "\n",
        "\n",
        "config = SegformerConfig.from_pretrained(MODEL_NAME,\n",
        "                        num_labels=len(class2idx), \n",
        "                        id2label=idx2class, \n",
        "                        label2id=class2idx,\n",
        "                        ignore_mismatched_sizes=True,\n",
        ")\n",
        "\n",
        "model2 = SegformerForSemanticSegmentation(config)\n",
        "# model_path = '/content/drive/MyDrive/datas/HuBMAP/model-sub3-fold{}.pth'.format(FOLD)\n",
        "model2.load_state_dict(torch.load(model_path))\n",
        "model2 = model2.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXTBuq48Ds18"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from transformers import SegformerFeatureExtractor\n",
        "feature_extractor = SegformerFeatureExtractor(reduce_labels=False, size=(WIDTH, HEIGHT))\n",
        "\n",
        "# print(pixel_values.shape)\n",
        "# image = Image.open('/content/hubmap/train_images/10651.tiff')\n",
        "# plt.imshow(image)\n",
        "\n",
        "model2.eval()\n",
        "dice = 0.0\n",
        "\n",
        "test_df = df[df['fold'] == FOLD].reset_index()\n",
        "for i in range(len(test_df)) :\n",
        "  id = test_df.id[i]\n",
        "  fname = \"/content/hubmap/train_images/{}.tiff\".format(id)\n",
        "  image = Image.open(fname)\n",
        "  encoding = feature_extractor(image, return_tensors=\"pt\")\n",
        "  pixel_values = encoding.pixel_values.cuda()\n",
        "  organ = test_df.organ[i]\n",
        "  rle = test_df.rle[i]\n",
        "  height, width = test_df.img_height[i], test_df.img_width[i]\n",
        "  idx = class2idx[organ]\n",
        "  with torch.no_grad():\n",
        "    outputs = model2(pixel_values=pixel_values)\n",
        "    upsampled_logits = nn.functional.interpolate(outputs['logits'],\n",
        "                # size=image.size[::-1], # (height, width)\n",
        "                (height, width),\n",
        "                mode='bilinear',\n",
        "                align_corners=False)\n",
        "    mask = upsampled_logits.argmax(dim=1)[0]\n",
        "    mask[mask != idx] = 0\n",
        "    mask[mask == idx] = 1\n",
        "\n",
        "  gt = rle2mask(rle, shape = (height, width))\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(image)\n",
        "  plt.imshow(mask.cpu().numpy(), alpha=0.3)\n",
        "  plt.title(\"{}-{}-{},{}\".format(id, organ, width, height))\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.imshow(image)\n",
        "  plt.imshow(gt, alpha=0.3)\n",
        "  plt.show()\n",
        "\n",
        "  dice += dice_coef(mask.cpu().numpy(), gt)\n",
        "  print(dice_coef(mask.cpu().numpy(), gt), dice/(i+1))\n",
        "\n",
        "print(\"DICE = \", dice/len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKAhOOjJJpFn"
      },
      "outputs": [],
      "source": [
        "len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_resize(w, p) :\n",
        "    a = 3000/512\n",
        "    target = int((w*p/0.4)/a + 0.5)\n",
        "    target = (target+31)//32 * 32\n",
        "    return target\n",
        "\n",
        "calc_resize(2023, 0.4945), calc_resize(3000, 0.4), calc_resize(160, 6.263), calc_resize(4500, 0.2290)"
      ],
      "metadata": {
        "id": "ueh5Lzd0OwXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install staintools\n",
        "!pip install spams"
      ],
      "metadata": {
        "id": "Hnkz6Fm1QCTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPpVDhSnUesu"
      },
      "outputs": [],
      "source": [
        "# 多くのカグラーは染色ツールを使用して、増強のために組織画像の色を着色しています。\n",
        "# 彼らは他のより重要な側面を見逃しています：テクスチャ（すなわち形態）\n",
        "import staintools\n",
        "\n",
        "# Read data\n",
        "to_augment = staintools.read_image(\"/content/hubmap/train_images/10044.tiff\")\n",
        "\n",
        "# Standardize brightness (optional, can improve the tissue mask calculation)\n",
        "# to_augment = staintools.LuminosityStandardizer.standardize(to_augment)\n",
        "\n",
        "# Stain augment\n",
        "augmentor = staintools.StainAugmentor(method='vahadane', sigma1=0.2, sigma2=0.2)\n",
        "\n",
        "augmentor.fit(to_augment)\n",
        "augmented_images = []\n",
        "for _ in range(2):\n",
        "    augmented_image = augmentor.pop()\n",
        "    augmented_images.append(augmented_image)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlJL_cOhiXHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(to_augment)\n",
        "plt.show()\n",
        "for e in augmented_images:\n",
        "  plt.imshow(e/255)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TdEpJu3FQAyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AsZoLWLMSn0e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SUB5-b5-train-segformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hgGXobp_m4d2avFBWuNCC0VMZKtInxXk",
      "authorship_tag": "ABX9TyMCVe3epz4uP7NLNrQQ5JLX",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}